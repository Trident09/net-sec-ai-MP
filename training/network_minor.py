# -*- coding: utf-8 -*-
"""Network_minor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y2Ry7c6o4Oy_lwKlcZkRbF2u1swU72aN
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

def load_data(data_dir):
    """
    Loads all CSV files from the specified directory, cleans column names,
    and combines them into a single DataFrame.

    Args:
    - data_dir (str): Path to the directory containing CSV files.

    Returns:
    - pd.DataFrame: Combined DataFrame with data from all CSV files.
    """
    logging.info(f"Loading data from directory: {data_dir}")

    # List all CSV files in the directory
    csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]

    if not csv_files:
        raise FileNotFoundError(f"No CSV files found in directory: {data_dir}")

    dataframes = []

    for file in csv_files:
        try:
            # Load each CSV file into a DataFrame
            df = pd.read_csv(file)

            # Clean column names by stripping any leading or trailing whitespace
            df.columns = df.columns.str.strip()

            # Append the DataFrame to the list
            dataframes.append(df)

            # Log the shape of the loaded DataFrame
            logging.info(f"Loaded {file} with shape {df.shape}")

        except Exception as e:
            # Log an error if loading fails
            logging.error(f"Failed to load {file}. Error: {str(e)}")
            continue

    # Combine all DataFrames into one
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        logging.info(f"Data loaded. Combined shape: {combined_df.shape}")
    else:
        raise ValueError("No valid dataframes to combine.")

    return combined_df

# Set up logging (optional, for better logging output)
logging.basicConfig(level=logging.INFO)

# Specify the path to your directory containing CSV files
data_dir = '/content/drive/MyDrive/Colab Notebooks/Files/'

# Load the data from the specified directory
try:
    df = load_data(data_dir)
    print(f"Data loaded successfully. Shape of the combined DataFrame: {df.shape}")
except Exception as e:
    logging.error(f"Error while loading data: {str(e)}")

def explore_data(df):
    """
    Function to explore a dataset by showing basic information,
    summary statistics, missing values, and visualizations.

    Args:
    - df (pd.DataFrame): The DataFrame to explore.
    """
    # Basic Information
    print("Basic Information:")
    df.info()
    print("\n")

    # Show the first 5 rows
    print("First 5 rows of data:")
    print(df.head())
    print("\n")

    # Summary statistics for numeric columns
    print("Summary statistics for numeric columns:")
    print(df.describe())
    print("\n")

    # Missing Values
    print("Missing values per column:")
    print(df.isnull().sum())
    print("\n")

    # Check for duplicates
    print("Number of duplicate rows:", df.duplicated().sum())
    print("\n")

    # Data Types of Columns
    print("Data types of columns:")
    print(df.dtypes)
    print("\n")

    # Correlation heatmap for numeric columns
    print("Correlation heatmap of numeric columns:")
    # Select only numeric columns for correlation calculation
    numeric_df = df.select_dtypes(include=['number'])
    corr = numeric_df.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Heatmap")
    plt.show()

    # Plot histograms for numeric columns
    print("Histograms of numeric columns:")
    # Select only numeric columns and replace infinite values with NaN
    numeric_df = df.select_dtypes(include=['number'])
    numeric_df = numeric_df.replace([np.inf, -np.inf], np.nan)  # Replace inf with NaN

    # Drop rows with any NaN values in numeric columns before plotting
    numeric_df.dropna().hist(bins=20, figsize=(12, 10))
    plt.tight_layout()
    plt.show()

# Call the explore_data function on your loaded DataFrame (df)
explore_data(df)

# Check for missing values
df.isnull().sum()

# Handle missing values by filling with the mean for numeric columns only
for col in df.select_dtypes(include=np.number).columns:
    df[col].fillna(df[col].mean(), inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# One-hot encode categorical columns
print(df.columns)

df = pd.get_dummies(df, columns=['Label'])

# Split into training and testing datasets
# Assuming 'df' is your DataFrame and the one-hot encoded columns for 'Label' exist
# Get a list of columns starting with 'Label_' (assuming the prefix used by pd.get_dummies)
label_columns = [col for col in df.columns if col.startswith('Label_')]

# Features (all columns except the one-hot encoded 'Label' columns)
X = df.drop(columns=label_columns)

# Target variable (the one-hot encoded 'Label' columns)
y = df[label_columns]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Replace infinite values with NaN in both training and testing data
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True) # This line was added

# Impute NaN values with the mean of each column for both training and testing data
for col in X_train.select_dtypes(include=np.number).columns:
    X_train[col].fillna(X_train[col].mean(), inplace=True)
    X_test[col].fillna(X_train[col].mean(), inplace=True)  #This line was added, and mean of training set is used

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the training data, and transform the test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Training data size:", X_train.shape)
print("Testing data size:", X_test.shape)

# Initialize the model
model = RandomForestClassifier(random_state=42)

# Train the model
model.fit(X_train_scaled, y_train)

print("Model training complete.")

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Detailed classification report
print(classification_report(y_test, y_pred))

import joblib

# Save the model
joblib.dump(model, 'trained_model.pkl')

# Load the model later
loaded_model = joblib.load('trained_model.pkl')